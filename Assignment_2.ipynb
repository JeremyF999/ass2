{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgZqSHiMoG0"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9414/Assignment2/blob/main/COMP9414-Assignment2.ipynb)\n",
        "\n",
        "# COMP9414 24T3 - Assignment 2 - Neural Networks, Decision Trees and Random Forests\n",
        "\n",
        "## UNSW Sydney\n",
        "\n",
        "Designed by Gustavo Batista.\n",
        "\n",
        "Last change: 20th October, 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXpdVgDmMoG2"
      },
      "source": [
        "Student name - zID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHo1y3Z2MoG2"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "**Submission deadline:** Friday, 8th November 2024, at 17:00:00 AEDT.\n",
        "\n",
        "**Submission:** You can submit your solution via the give system using the command ``give ass2 ass2.ipynb``.\n",
        "\n",
        "**Instructions:**\n",
        "* This is an **individual** assignment.\n",
        "* Write your name and zID on the top of this Jupyter Notebook.\n",
        "* You can only use the libraries listed in this notebook\n",
        "* You can reuse any piece of source code developed in the tutorials.\n",
        "* Do not modify the existing code in this notebook except to answer the questions. The cells that should be modified are indicated.\n",
        "* If you want to submit additional code (e.g., for generating plots), write it at the end of the notebook.\n",
        "* This notebook is worth **75** marks and will be rescaled to **25** marks.\n",
        "\n",
        "**Late Submission Policy:** The penalty is a 5% reduction of the assignment value (i.e. 1.25 marks) in the mark per day. For example, if an assignment gets an on-time mark of $20/25$ and is submitted three days late, it will receive a mark reduction of $3*1.25 = 3.75$, so the assignment will get $16.25$ after three days. After five days, the assignment will receive a mark reduction of $100\\%$.\n",
        "\n",
        "\n",
        "**Plagiarism:**\n",
        "\n",
        "Remember that ALL work submitted for this assignment must be your own work, and no sharing or copying of code or answers is allowed. You may discuss the assignment with other students but must not collaborate to develop answers to the questions. You may use code from the Internet only with suitable attribution of the source. You may not use ChatGPT or any similar software to generate any part of your explanations, evaluations or code. Do not use public code repositories on sites such as GitHub or file-sharing sites such as Google Drive to save any part of your work &ndash; make sure your code repository or cloud storage is private, and do not share any links. This also applies after you have finished the course, as we do not want next year’s students accessing your solution, and plagiarism penalties can still apply after the course has finished.\n",
        "\n",
        "All submitted assignments will be run through plagiarism detection software to detect similarities to other submissions, including from past years. You should **carefully** read the UNSW policy on academic integrity and plagiarism (linked from the course web page), noting, in particular, that collusion (working together on an assignment or sharing parts of assignment solutions) is a form of plagiarism.\n",
        "\n",
        "Finally, do not use any contract cheating “academies” or online “tutoring” services. This counts as serious misconduct with heavy penalties up to automatic failure of the course with 0 marks and expulsion from the university for repeat offenders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h61x0u3XMoG3"
      },
      "source": [
        "## Technical prerequisites\n",
        "\n",
        "These are the libraries you are allowed to use. No other libraries will be accepted. Make sure you are using Python 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ANLYJm31AoJ",
        "outputId": "7c1a76e4-cae6-43b1-a8aa-51b84cfd4e95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras_tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tzTQYskJMoG4"
      },
      "outputs": [],
      "source": [
        "# These are the allowed libraries. You can add other libraries used in the tutorials.\n",
        "\n",
        "# Common Python libraries\n",
        "import math\n",
        "import copy\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Scikit-Learn libraries for data preprocessing and model assessment\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Libraries for the tree models\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "# Scikit-learn libraries for hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Tensorflow/keras libraries for shallow and deep-learning models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "# Keras Tuner libraries for hyperparameter tuning\n",
        "from keras_tuner import HyperModel\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "# Libraries to present results in tabular format\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFosjS6MoG5"
      },
      "source": [
        "This assignment compares three Machine Learning approaches: Neural Networks, Decision Trees, and Random Forests. We will assess these approaches in five benchmark datasets with diverse characteristics.\n",
        "\n",
        "We would like to test a few hypotheses based on common Machine Learning wisdom and misconceptions.\n",
        "\n",
        "1. Neural networks are the best general classifiers regarding prediction quality (accuracy, error rate, precision, recall, etc.).\n",
        "2. Neural networks are time-consuming for training as fitting model parameters is slow and has many hyperparameters.\n",
        "3. Random forests are an excellent compromise between classification performance and hyperparameter tuning. They can often provide competitive accuracy without requiring much hyperparameter tuning.\n",
        "4. Neural networks are data-hungry and perform poorly in small datasets.\n",
        "5. Decision trees offer model interpretability but are not competitive in accuracy.\n",
        "6. Neural networks are the best models when learning from unstructured data such as images.\n",
        "7. Random forests are the best models when learning from structured data such as a tabular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJh3_2EqMoG5"
      },
      "source": [
        "## Task 0 - Datasets description, downloading and loading the data into a Pandas dataframe\n",
        "\n",
        "We have selected five publicly available benchmark datasets:\n",
        "\n",
        "1. **UCI adult income dataset.** This is a binary classification dataset in which we want to predict if a person earns more than $50k/year. It is a mid-size dataset (48K examples) with 14 features of mixed data types (categorical and continuous) with missing values.\n",
        "\n",
        "2. **Forest cover type dataset.** This is a large multi-class dataset with 580K examples and 54 features of mixed types. The objective is to predict the type of forest cover based on features such as soil type, elevation, and slope.\n",
        "\n",
        "3. **California housing prices**. This is a regression problem in which we want to predict housing prices based on numerical features, such as population, median income and location. It has 20K instances and nine features.\n",
        "\n",
        "4. **Fashion MNIST dataset**. It is an image classification dataset that is very similar to MNIST. Images are $28 \\times 28$ grayscale pixels. The objective is to classify ten different types of clothing. It has 60k training and 10K test instances.\n",
        "\n",
        "5. **Credit card fraud detection**. This is a binary classification dataset for detecting fraudulent transactions in credit card data. It is highly imbalanced, meaning that most transactions are normal, with some rare fraud cases. It has 284K instances and 30 numerical features.\n",
        "\n",
        "This table summarises the datasets.\n",
        "\n",
        "| Dataset                          | Problem Type        | Feature Type                          | Size        | Notable Challenge                                    |\n",
        "|-----------------------------------|---------------------|---------------------------------------|-------------|------------------------------------------------------|\n",
        "| **UCI Adult Income**              | Binary Classification| Categorical and Numerical             | 48,000      | Mix of feature types with missing values           |\n",
        "| **Forest Cover Type**             | Multi-class Classification | Categorical and Numerical       | 580,000     | Large dataset with mix of feature types      |\n",
        "| **California Housing Prices**     | Regression          | Numerical                              | 20,000      | Regression task      |\n",
        "| **Fashion MNIST**                 | Multi-class Classification (Image)| Image (grayscale)        | 60,000      | Weak features in the form of individual pixels brightness       |\n",
        "| **Credit Card Fraud Detection**   | Binary Classification (Imbalanced)| Numerical                | 284,000     | Highly imbalanced dataset        |\n",
        "\n",
        "Let's start by downloading the data from GitHub. The cell below will download and save the data into a local ``data`` folder. We will use the data later to train and assess our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZelgEffMoG5",
        "outputId": "9ea0f50e-8bd0-437a-9000-f75368b76642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading adult.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/adult/adult.zip...\n",
            "Downloading covertype.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/covertype/covertype.zip...\n",
            "Downloading california_housing.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/california_housing/california_housing.zip...\n",
            "Downloading creditcard.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/creditcard/creditcard.zip...\n",
            "Downloading fashion_mnist.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/fashion_mnist/fashion_mnist.zip...\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It downloads and unzips the datasets to your local disk.\n",
        "\n",
        "def download_and_extract(url, extract_to):\n",
        "    \"\"\"\n",
        "    Download a zip file from the URL and extract it to the specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the zip file to download.\n",
        "    - extract_to (str): The directory where the zip file's contents will be extracted.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Get the file, dataset names from the URL\n",
        "    zip_filename = url.split(\"/\")[-1]\n",
        "    dataset_name = zip_filename.split(\".\")[0]\n",
        "\n",
        "    # Each dataset will have its folder\n",
        "    extract_to = extract_to + \"/\" + dataset_name\n",
        "\n",
        "    # Download the zip file\n",
        "    print(f\"Downloading {zip_filename} from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    with open(zip_filename, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Create the extraction directory if it doesn't exist\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    # Remove the zip files\n",
        "    os.remove(zip_filename)\n",
        "\n",
        "# These are the URLs to the datasets. We have hosted the data on GitHub.\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/adult/adult.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/covertype/covertype.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/california_housing/california_housing.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/creditcard/creditcard.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/fashion_mnist/fashion_mnist.zip\",\n",
        "]\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "    download_and_extract(url, \"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytPliysiMoG5"
      },
      "source": [
        "### Loading data into pandas\n",
        "\n",
        "The datasets are well-diversified in size (number of examples and features), number of class labels, feature types (continuous and discrete), class distribution, and presence of missing data.\n",
        "\n",
        "All datasets have pre-defined training and testing splits. We will use the training set to train the models and choose hyperparameters. You may further split the training set into training and validation sets. The test set should only be used to assess and compare the models.\n",
        "\n",
        "The next cell has a supporting function that loads a specified dataset training and test sets into a pandas' dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1NyKxYfMoG6",
        "outputId": "1b2e9622-b58b-4254-e0c6-8a348b29b883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data/adult...\n",
            "Train Features Shape: (32561, 14), Train Labels Shape: (32561, 1)\n",
            "Test Features Shape: (16281, 14), Test Labels Shape: (16281, 1)\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It is a helper function that loads data from this into a Pandas dataframe.\n",
        "\n",
        "def load_train_test_data(path):\n",
        "    \"\"\"\n",
        "    Loads the train and test CSV files and returns them split into features (X) and labels (y).\n",
        "\n",
        "    Parameters:\n",
        "    - path (str): Path to the train and test CSV files.\n",
        "\n",
        "    Returns:\n",
        "    - X_train (DataFrame): Features of the training dataset.\n",
        "    - y_train (DataFrame): Labels of the training dataset.\n",
        "    - X_test (DataFrame): Features of the test dataset.\n",
        "    - y_test (DataFrame): Labels of the test dataset.\n",
        "    \"\"\"\n",
        "    # Load the training and testing data\n",
        "    train_df = pd.read_csv(f\"{path}/train.csv\")\n",
        "    test_df = pd.read_csv(f\"{path}/test.csv\")\n",
        "\n",
        "    # Select class label columns (those starting with 'Target')\n",
        "    y_train = train_df.filter(regex='^Target')\n",
        "    y_test = test_df.filter(regex='^Target')\n",
        "\n",
        "    # Select feature columns (all columns except the ones with 'Target' prefix)\n",
        "    X_train = train_df.drop(columns=y_train.columns)\n",
        "    X_test = test_df.drop(columns=y_test.columns)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Example usage:\n",
        "path = \"data/adult\"\n",
        "print(f\"Loading {path}...\")\n",
        "X_train, y_train, X_test, y_test = load_train_test_data(path)\n",
        "\n",
        "print(f\"Train Features Shape: {X_train.shape}, Train Labels Shape: {y_train.shape}\")\n",
        "print(f\"Test Features Shape: {X_test.shape}, Test Labels Shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hArM4VI2MoG6"
      },
      "source": [
        "## Task 1 [14 Marks] - Data preprocessing\n",
        "\n",
        "Your first task is to preprocess the datasets. Preprocessing usually involves data cleaning and transformation to improve data quality and prepare the data for the specific requirements of the learning approaches.\n",
        "\n",
        "For data preparation, we have the following tasks:\n",
        "1. **Missing imputation (all models)**: The adult dataset has missing values, and none of our learning algorithm implementations can directly handle missing data. Two missing data treatments are eliminating the rows with missing data or replacing the missing values with estimated ones. *Mean imputation*, as the name suggests, replaces missing values with the attribute mean, median (continuous features) or mode (discrete features). These statistics must only be estimated in the training set.\n",
        "2. **Feature encoding (all models)**: Neural networks, tree and random forest implementations available in the Scikit-Learn library do not handle categorical attributes directly. Therefore, these attributes need to be converted into numerical attributes. Although several encoding approaches exist, we will use one-hot encoding, as it is simple and recommended for categorical features with a small cardinality.\n",
        "3. **Class attribute encoding (neural networks only)**: Neural networks also need a one-hot encoding for the class attribute. This step is not necessary for the tree models.\n",
        "4. **Rescaling attribute values (neural networks only)**: The neural network's training benefits from rescaling the attribute values. In this task, we will convert each attribute to a number in the 0 to 1 range by using a simple linear rescaling: $x_s = \\frac{x-min_f}{max_f-min_f}$, where $x_s$ is the recalled $x$ value, $min_f$ is the minimum and $max_f$ the maximum values for feature $f$ in the training data.\n",
        "\n",
        "Tree models typically do not use class encoding and rescaling. The reason is twofold: first, this preprocessing does not help these models fit better parameters; second, tree models are known for their interpretability, and these manipulations create models that are not easier and often harder to understand. Feature encoding is also unnecessary for many tree model implementations, including the well-known [XGBoost](https://xgboost.readthedocs.io/en/stable/) and [LightGBM](https://lightgbm.readthedocs.io/en/stable/). Unfortunately, the Scikit-Learn implementation of tree models does not support categorical attributes.\n",
        "\n",
        "**Warning**: Leaking information from the test set to the training set, even if such information is aggregated data such as means, maximums, and minimums, is considered a serious methodological error. For instance, mean imputation should use the mean only in the training set. Similarly, the maximum and minimum for attribute rescaling should be calculated in the training set. Consequently, we may see values outside the range of 0-1 in the rescaled test set. This mimics the situation in which we find extreme values after the model deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lya9wWypMoG6"
      },
      "source": [
        "### Task 1.1 [6 Marks] - Missing data removal or imputation\n",
        "\n",
        "Create a function ``missing_data(X_train, X_test)`` that imputes missing values in the dataframes `X_train` and `X_test`. When the function returns, both dataframes should have no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SXbkk2DcMoG7"
      },
      "outputs": [],
      "source": [
        "def missing_data(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Impute missing values in the train and test DataFrames using median/mode imputation.\n",
        "    Missing data statistics are only estimated on the training set and applied to the training and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - X_test (DataFrame): Test features.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_filled (DataFrame): Training features with no missing values.\n",
        "    - X_test_filled (DataFrame): Test features with no missing values.\n",
        "    \"\"\"\n",
        "    # Ensure DataFrames are not empty\n",
        "    if X_train.empty or X_test.empty:\n",
        "        raise ValueError(\"X_train or X_test is empty. Check data loading process.\")\n",
        "\n",
        "    # Identify numeric and categorical columns\n",
        "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # If no numeric or categorical columns, raise an error\n",
        "    if numeric_cols.empty and categorical_cols.empty:\n",
        "        raise ValueError(\"No valid numeric or categorical columns found in the data.\")\n",
        "\n",
        "    # Create imputers for numeric and categorical data\n",
        "    numeric_imputer = SimpleImputer(strategy='median')\n",
        "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "    # Impute numeric columns\n",
        "    if not numeric_cols.empty:\n",
        "        X_train_numeric = pd.DataFrame(\n",
        "            numeric_imputer.fit_transform(X_train[numeric_cols]),\n",
        "            columns=numeric_cols,\n",
        "            index=X_train.index\n",
        "        )\n",
        "        X_test_numeric = pd.DataFrame(\n",
        "            numeric_imputer.transform(X_test[numeric_cols]),\n",
        "            columns=numeric_cols,\n",
        "            index=X_test.index\n",
        "        )\n",
        "    else:\n",
        "        X_train_numeric, X_test_numeric = X_train[numeric_cols], X_test[numeric_cols]\n",
        "\n",
        "    # Impute categorical columns\n",
        "    if not categorical_cols.empty:\n",
        "        X_train_categorical = pd.DataFrame(\n",
        "            categorical_imputer.fit_transform(X_train[categorical_cols]),\n",
        "            columns=categorical_cols,\n",
        "            index=X_train.index\n",
        "        )\n",
        "        X_test_categorical = pd.DataFrame(\n",
        "            categorical_imputer.transform(X_test[categorical_cols]),\n",
        "            columns=categorical_cols,\n",
        "            index=X_test.index\n",
        "        )\n",
        "    else:\n",
        "        X_train_categorical, X_test_categorical = X_train[categorical_cols], X_test[categorical_cols]\n",
        "\n",
        "    # Combine numeric and categorical columns back\n",
        "    X_train_filled = pd.concat([X_train_numeric, X_train_categorical], axis=1)\n",
        "    X_test_filled = pd.concat([X_test_numeric, X_test_categorical], axis=1)\n",
        "\n",
        "    return X_train_filled, X_test_filled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPwU-tZUMoG7"
      },
      "source": [
        "### Task 1.2 [4 Marks] - Feature and class encoding\n",
        "\n",
        "Let's implement a function ``encoding(X_train, X_test)`` that creates one-hot encodings for all categorical attributes. All categorical attributes are encoded as one-hot numeric features when the function returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7jnCCQnMoG7",
        "outputId": "9659afa1-974d-4be7-d324-17d8f0abf1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train after encoding: (32561, 97)\n",
            "Shape of X_test after encoding: (16281, 97)\n"
          ]
        }
      ],
      "source": [
        "def encoding(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Encodes categorical features and class labels into one-hot numeric features.\n",
        "    Ensure that you have a consistent encoding across training and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - X_test (DataFrame): Test features.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_encoded (DataFrame): One-hot encoded training features.\n",
        "    - X_test_encoded (DataFrame): One-hot encoded test features.\n",
        "    \"\"\"\n",
        "    # Perform one-hot encoding using pandas' get_dummies\n",
        "    X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
        "    X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "    # Align test set columns with the training set\n",
        "    X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
        "\n",
        "    return X_train_encoded, X_test_encoded\n",
        "\n",
        "\n",
        "X_train_encoded, X_test_encoded = encoding(X_train, X_test)\n",
        "\n",
        "print(\"Shape of X_train after encoding:\", X_train_encoded.shape)\n",
        "print(\"Shape of X_test after encoding:\", X_test_encoded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ2n_-KTMoG7"
      },
      "source": [
        "#### Task 1.3 [4 Marks] - Rescaling attributes\n",
        "\n",
        "To conclude the pre-processing task, let's create a function ``rescale(X_train, X_test)`` that rescales all continuous attributes so that each attribute is between 0 and 1. When the function returns, all numerical attributes should be rescaled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WRr-NM2MoG7",
        "outputId": "2979c246-f81e-4390-b89d-1b782acedd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train after encoding: (32561, 97)\n",
            "Shape of X_test after encoding: (16281, 97)\n"
          ]
        }
      ],
      "source": [
        "def encoding(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Encodes categorical features and class labels into one-hot numeric features.\n",
        "    Ensure that you have a consistent encoding across training and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - X_test (DataFrame): Test features.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_encoded (DataFrame): One-hot encoded training features.\n",
        "    - X_test_encoded (DataFrame): One-hot encoded test features.\n",
        "    \"\"\"\n",
        "    # Perform one-hot encoding using pandas' get_dummies\n",
        "    X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
        "    X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "    # Align test set columns with the training set\n",
        "    X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
        "\n",
        "    return X_train_encoded, X_test_encoded\n",
        "\n",
        "def rescale(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Rescales all continuous attributes in the train and test datasets to be in the range [0, 1].\n",
        "    Rescaling statistics should only be estimated on the training set and applied to the training and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - X_test (DataFrame): Test features.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_rescaled (DataFrame): Rescaled training features.\n",
        "    - X_test_rescaled (DataFrame): Rescaled test features.\n",
        "    \"\"\"\n",
        "    # Initialize MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform both training and testing data\n",
        "    X_train_rescaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "    X_test_rescaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "    return X_train_rescaled, X_test_rescaled\n",
        "\n",
        "X_train_encoded, X_test_encoded = encoding(X_train, X_test)\n",
        "\n",
        "print(\"Shape of X_train after encoding:\", X_train_encoded.shape)\n",
        "print(\"Shape of X_test after encoding:\", X_test_encoded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zmsE9DXMoG8"
      },
      "source": [
        "### Preprocessing the datasets\n",
        "\n",
        "In the cell below, we will call your functions to preprocess the datasets. We will create two versions of each dataset: the first is suitable for the tree models and will have no missing values and encoded attributes. The second will have no missing values, encoded categorical and class features, and numeric features rescaled. We will save these datasets for use later. The datasets pre-processed for trees will be saved in a ``tree`` folder. The datasets for neural networks will be saved in a ``nn`` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpr_4QQ4MoG8",
        "outputId": "10abc7ff-6d21-414b-cdbf-b84cb89d85d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "Processing dataset: covertype\n",
            "Processing dataset: california_housing\n",
            "Processing dataset: fashion_mnist\n",
            "Processing dataset: creditcard\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It calls your pre-processing functions and saves the preprocessed datasets on disk.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"california_housing\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(\"data/\" + dataset)\n",
        "\n",
        "    # Preprocessing for tree-based models\n",
        "    tree_path = f\"data/{dataset}/tree\"\n",
        "    if not os.path.exists(tree_path):\n",
        "        os.makedirs(tree_path)\n",
        "\n",
        "    # Handle missing data\n",
        "    X_train, X_test = missing_data(X_train, X_test)\n",
        "\n",
        "    # Apply encoding features\n",
        "    X_train_encoded, X_test_encoded = encoding(X_train, X_test)\n",
        "\n",
        "    # Concatenate X and y for train and test data.\n",
        "    # For decision trees, we do not encode the class attribute\n",
        "    train_tree = pd.concat([X_train_encoded, y_train.reset_index(drop=True)], axis=1)\n",
        "    test_tree = pd.concat([X_test_encoded, y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save tree-preprocessed datasets\n",
        "    train_tree.to_csv(f\"{tree_path}/train.csv\", index=False)\n",
        "    test_tree.to_csv(f\"{tree_path}/test.csv\", index=False)\n",
        "\n",
        "    # Preprocessing for neural networks\n",
        "    nn_path = f\"data/{dataset}/nn\"\n",
        "    if not os.path.exists(nn_path):\n",
        "        os.makedirs(nn_path)\n",
        "\n",
        "    # Apply encoding class attribute. For a regression dataset, the next line should do nothing\n",
        "    y_train_encoded, y_test_encoded = encoding(y_train, y_test)\n",
        "\n",
        "    # Rescale the features\n",
        "    X_train_rescaled, X_test_rescaled = rescale(X_train_encoded, X_test_encoded)\n",
        "\n",
        "    # Concatenate X and y for train and test data\n",
        "    train_nn = pd.concat([X_train_rescaled, y_train_encoded.reset_index(drop=True)], axis=1)\n",
        "    test_nn = pd.concat([X_test_rescaled, y_test_encoded.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save nn-preprocessed datasets\n",
        "    train_nn.to_csv(f\"{nn_path}/train.csv\", index=False)\n",
        "    test_nn.to_csv(f\"{nn_path}/test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChkLtQZMMoG8"
      },
      "source": [
        "## Task 2 - [16 Marks] Model Training\n",
        "\n",
        "We have the data ready, and in this task, we will train some initial models for each dataset. We will refine the models later, but for now, we will create a swallow model for the neural network. The decision tree and the random forest models will use Scikit-Learn's default hyperparameter values for these models.\n",
        "\n",
        "The neural network will have three layers: the input layer ($i$), one hidden layer ($h$) and one output layer ($o$). We will use a simple rule-of-thumb for the number of units in the hidden layer: $D_h = \\sqrt{D_i * D_o}$. The other hyperparameters are similar to the ones used in the Week 07 tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb-ikg9eMoG8"
      },
      "source": [
        "### Task 2.1 [4 Marks] - Shallow neural net for classification\n",
        "\n",
        "Create a function ``train_shallow_net_class(X_train, y_train)`` that trains a shallow neural net for classification using the training data ``X_train`` and labels ``y_train``. Use the following hyperparameters:\n",
        "1. A single hidden layer with $D_h = \\text{round}(\\sqrt{D_i * D_o})$ units.\n",
        "2. ReLU activation in the hidden layer and softmax on the output layer.\n",
        "3. Categorical cross-entropy as loss function.\n",
        "4. Train for 30 epochs.\n",
        "5. Batch size of 32 instances.\n",
        "6. Validation split of 20% of the training data.\n",
        "7. Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GRb1a9sgMoG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "def train_shallow_net_class(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a shallow neural net for classification problems with one hidden layer using the training data.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training labels (one-hot encoded for classification).\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Keras neural network model.\n",
        "    \"\"\"\n",
        "    # Calculate the number of units in the hidden layer\n",
        "    Di = X_train.shape[1]  # Number of input features\n",
        "    Do = y_train.shape[1]  # Number of output classes\n",
        "    Dh = round(np.sqrt(Di * Do))  # Hidden layer size\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential([\n",
        "        Dense(Dh, activation='relu', input_shape=(Di,)),  # Hidden layer\n",
        "        Dense(Do, activation='softmax')  # Output layer\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss=CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGGReA0bMoG9"
      },
      "source": [
        "The next cell will call your function to train a shallow model for each classification dataset, compute the training time, and test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVLelF6WMoG9",
        "outputId": "e6621a99-6f66-4251-fea4-aecfd5fb4ae8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing dataset: adult\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(32, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m803/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2435 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2435 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 2/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2392 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 3/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2413 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 4/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.2405 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 5/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2427 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 6/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2385 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 7/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2348 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 8/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2424 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 9/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.2431 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 10/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2395 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 11/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2410 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 12/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2337 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 13/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2353 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 14/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2413 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 15/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.2403 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 16/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2355 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 17/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2338 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 18/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2346 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 19/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2439 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 20/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2357 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 21/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2438 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 22/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2383 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 23/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2362 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 24/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2376 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 25/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2414 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 26/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2452 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 27/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2430 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 28/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2393 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 29/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2430 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 30/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2348 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2366 - loss: 0.0000e+00\n",
            "Test error rate: 0.7638\n",
            "Runtime to train the model: 59.431533098220825 seconds\n",
            "Processing dataset: covertype\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - accuracy: 0.6297 - loss: 0.7745 - val_accuracy: 0.6519 - val_loss: 0.6911\n",
            "Epoch 2/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - accuracy: 0.6524 - loss: 0.6846 - val_accuracy: 0.6508 - val_loss: 0.6855\n",
            "Epoch 3/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.6439 - loss: 0.6903 - val_accuracy: 0.6558 - val_loss: 0.6830\n",
            "Epoch 4/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.6364 - loss: 0.7032 - val_accuracy: 0.6551 - val_loss: 0.6788\n",
            "Epoch 5/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.6291 - loss: 0.7176 - val_accuracy: 0.6136 - val_loss: 0.7059\n",
            "Epoch 6/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.6261 - loss: 0.7310 - val_accuracy: 0.6239 - val_loss: 0.7051\n",
            "Epoch 7/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.6200 - loss: 0.7483 - val_accuracy: 0.6475 - val_loss: 0.6978\n",
            "Epoch 8/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.6201 - loss: 0.7667 - val_accuracy: 0.5662 - val_loss: 0.7985\n",
            "Epoch 9/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.6157 - loss: 0.7808 - val_accuracy: 0.5891 - val_loss: 1.0605\n",
            "Epoch 10/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.6126 - loss: 0.7952 - val_accuracy: 0.6346 - val_loss: 0.7120\n",
            "Epoch 11/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.6118 - loss: 0.8137 - val_accuracy: 0.6102 - val_loss: 0.7546\n",
            "Epoch 12/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.6106 - loss: 0.8225 - val_accuracy: 0.5052 - val_loss: 1.2991\n",
            "Epoch 13/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - accuracy: 0.6038 - loss: 0.8541 - val_accuracy: 0.5918 - val_loss: 0.7810\n",
            "Epoch 14/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.5983 - loss: 0.8840 - val_accuracy: 0.5640 - val_loss: 0.8868\n",
            "Epoch 15/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5980 - loss: 0.8997 - val_accuracy: 0.5131 - val_loss: 1.0618\n",
            "Epoch 16/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.5978 - loss: 0.9132 - val_accuracy: 0.5482 - val_loss: 0.9589\n",
            "Epoch 17/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.5924 - loss: 0.9352 - val_accuracy: 0.5713 - val_loss: 0.9452\n",
            "Epoch 18/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - accuracy: 0.5914 - loss: 0.9672 - val_accuracy: 0.6287 - val_loss: 0.7594\n",
            "Epoch 19/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.5917 - loss: 0.9739 - val_accuracy: 0.4883 - val_loss: 1.1798\n",
            "Epoch 20/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5854 - loss: 1.0120 - val_accuracy: 0.5348 - val_loss: 1.0507\n",
            "Epoch 21/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5911 - loss: 1.0005 - val_accuracy: 0.5573 - val_loss: 1.0145\n",
            "Epoch 22/30\n",
            "\u001b[1m6400/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5913 - loss: 1.0184"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your shallow model using the classification datasets.\n",
        "\n",
        "results = defaultdict(dict)\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_shallow_net_class(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net\"][dataset][\"Prediction quality\"] =  1 - test_accuracy         # Error rate = 1 - accuracy\n",
        "    results[\"Neural Net\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKuR1UBXMoG9"
      },
      "source": [
        "### Task 2.2 [4 Marks] - Shallow neural net for regression\n",
        "\n",
        "Like the previous task, create a function ``train_shallow_net_regression(X_train, y_train)`` that trains a shallow neural net for regression using the training data ``X_train`` and labels ``y_train``. Use the following hyperparameters:\n",
        "1. A single hidden layer with $D_h = \\text{round}(\\sqrt{D_i * D_o})$ units.\n",
        "2. ReLU activation in the hidden layer and linear on the output layer.\n",
        "3. MSE loss function.\n",
        "4. Train for 30 epochs.\n",
        "5. Batch size of 32 instances.\n",
        "6. Validation split of 20% of the training data.\n",
        "7. Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW_GnsgeMoG9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "def train_shallow_net_regression(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a shallow neural net with one hidden layer using the training data.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame or ndarray): Training features.\n",
        "    - y_train (DataFrame or ndarray): Training labels.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Keras neural network model.\n",
        "    \"\"\"\n",
        "    # Calculate the number of units in the hidden layer\n",
        "    Di = X_train.shape[1]  # Number of input features\n",
        "    Do = 1  # Output is a single value for regression\n",
        "    Dh = round(np.sqrt(Di * Do))  # Hidden layer size\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential([\n",
        "        Dense(Dh, activation='relu', input_shape=(Di,)),  # Hidden layer\n",
        "        Dense(Do, activation='linear')  # Output layer for regression\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss=MeanSquaredError(),\n",
        "                  metrics=['mse'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14D14o7AMoG9"
      },
      "source": [
        "Once again, we will run your code for each regression dataset. This assignment has only one of such datasets, but we will keep a similar code we implemented before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMOtXYRhMoG9"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your shallow model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_shallow_net_regression(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_mse = model.evaluate(X_test, y_test)\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Neural Net\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-88-yYCMoG-"
      },
      "source": [
        "### Task 2.3 [2 Marks] - Decision tree models for classification\n",
        "\n",
        "Implement a function ``train_classification_tree(X_train, y_train)`` that trains a decision tree model using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn decision tree classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqYVId8WMoG-"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def train_classification_tree(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Decision Tree for classification.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame or ndarray): Training features.\n",
        "    - y_train (DataFrame or ndarray): Training class labels.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Decision Tree Classifier.\n",
        "    \"\"\"\n",
        "    # Initialize the Decision Tree Classifier with default hyperparameters\n",
        "    model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ximajKMaMoG-"
      },
      "source": [
        "The code below executes the tree models and records the test accuracy and training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqdxjIvvMoG-"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your decision tree model using the classification datasets.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training decision tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_classification_tree(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Decision Tree\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl_mtIhLMoG-"
      },
      "source": [
        "### Task 2.4 [2 Marks] - Decision tree models for regression\n",
        "\n",
        "Implement a function ``train_regression_tree(X_train, y_train)`` that trains a regression tree model using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn decision tree regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJD0xKwtMoG-"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def train_regression_tree(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Decision Tree for regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Decision Tree Regressor.\n",
        "    \"\"\"\n",
        "    ... # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NUnNPtBMoG_"
      },
      "source": [
        "The code below executes the regression tree models and saves the running time and test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MueX5JWMoG_"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your regression tree model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training regression tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_regression_tree(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Decision Tree\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw-R2wl2MoG_"
      },
      "source": [
        "### Task 2.5 [2 Marks] - Random forest models for classification\n",
        "\n",
        "Implement a function ``train_classification_forest(X_train, y_train)`` that trains a random forest model for classification using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoYSpmaxMoG_"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def train_regression_tree(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Decision Tree for regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame or ndarray): Training features.\n",
        "    - y_train (DataFrame or ndarray): Training target values.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Decision Tree Regressor.\n",
        "    \"\"\"\n",
        "    # Initialize the Decision Tree Regressor with default hyperparameters\n",
        "    model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih_8YMZ9MoG_"
      },
      "source": [
        "The code below executes the randon forest models and records the training time and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZCatgLrMoHF"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the classification datasets.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_classification_forest(X_train, np.array(y_train).ravel())\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Random Forest\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Xl9CYXMoHG"
      },
      "source": [
        "### Task 2.6 [2 Marks] - Random Forest Models for Regression\n",
        "\n",
        "Finally, implement a function ``train_regression_forest(X_train, y_train)`` that trains a random forest model for regression using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn random forest regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXfX1o7HMoHG"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def train_regression_forest(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Random Forest for regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame or ndarray): Training features.\n",
        "    - y_train (DataFrame or ndarray): Training target values.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Random Forest Regressor.\n",
        "    \"\"\"\n",
        "    # Initialize the Random Forest Regressor with default hyperparameters\n",
        "    model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ4ltwcIMoHG"
      },
      "source": [
        "The code below executes the random forest models for regression and records the training time and mean squared error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4glV-ziiMoHG"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_regression_forest(X_train, np.array(y_train).ravel())\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Random Forest\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUWaQt14MoHG"
      },
      "source": [
        "### Summarising the Results\n",
        "\n",
        "Congratulations, we have reached the end of Task 2. The next cell will summarise the results obtained in a single table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEbyyYhnMoHH"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It summarises the results in a tabular format\n",
        "\n",
        "def format_values(row):\n",
        "    \"\"\"Format numeric values to 4 decimal places.\"\"\"\n",
        "    return {k: f\"{v:.4f}\" if isinstance(v, float) else v for k, v in row.items()}\n",
        "\n",
        "def print_results_table(results):\n",
        "    \"\"\"\n",
        "    Converts a nested dictionary of results into a table and prints it with separation lines for datasets.\n",
        "    \"\"\"\n",
        "    # Flatten the nested dictionary into a list of rows\n",
        "    flattened_data = [\n",
        "        {\"Dataset\": dataset, \"Model\": model, **metrics}\n",
        "        for model, datasets in results.items()\n",
        "        for dataset, metrics in datasets.items()\n",
        "    ]\n",
        "\n",
        "    # Sort the data by the \"Dataset\" column\n",
        "    flattened_data_sorted = sorted(flattened_data, key=lambda x: x[\"Dataset\"])\n",
        "\n",
        "    # Add separator rows between datasets\n",
        "    formatted_data = []\n",
        "    previous_dataset = None\n",
        "    for row in flattened_data_sorted:\n",
        "        if previous_dataset and row[\"Dataset\"] != previous_dataset:\n",
        "            # Insert a separator row\n",
        "            formatted_data.append({key: \"----\" for key in row.keys()})\n",
        "        formatted_data.append(format_values(row))\n",
        "        previous_dataset = row[\"Dataset\"]\n",
        "\n",
        "    # Extract headers\n",
        "    headers = list(formatted_data[0].keys())\n",
        "\n",
        "    # Generate and print the table\n",
        "    table = tabulate(formatted_data, headers=\"keys\", tablefmt=\"pretty\", missingval=\"N/A\")\n",
        "    print(table)\n",
        "\n",
        "print_results_table(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ3Q9myuMoHH"
      },
      "source": [
        "## Task 3 [32 Marks] - Hyperparameter optimisation\n",
        "\n",
        "So far, we have used a fixed set of hyperparameters, but it is unclear if they are a good choice for our datasets. We will use Keras Tuner and Scikit Learn libraries to test different hyperparameter combinations. We will start with the Neural Net models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kPW4h8xMoHH"
      },
      "source": [
        "### Task 3.1 [8 Marks] - Hyperparameter optimisation for classification neural nets\n",
        "\n",
        "Create a function ``tune_train_classification_net(X_train, y_train, n_iter, project_name)`` that uses Keras tuner's ``RandomSearch`` to optimise the hyperparameters of a neural net model. ``X_train`` and ``y_train`` are pandas dataframes with the training data and labels. ``n_iter`` is the maximum number of iterations in the random search. ``project_name`` is an identifier used by Keras tuner to save the results on disk.\n",
        "\n",
        "You have the freedom to choose your hyperparameter search space. Here are some suggestions based on the tutorials:\n",
        "1. Depth. To make your model deeper, test a larger number of hidden layers, up to 3.\n",
        "2. Width. Try different combinations of numbers of neurons per layer. For instance, you can try from $D_h / 2$ to $D_h * 2$.\n",
        "3. Activation functions. ReLU, TANH and Sigmoid are common choices.\n",
        "4. Optimiser. Adam and SGD.\n",
        "5. Learning rate. A typical range is 1e-4 to 1e-2.\n",
        "\n",
        "Your function should return the Keras model that achieved the best performance in a validation set of 20% of the training data. Average performance over three runs (``executions_per_trial=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcHRG122MoHH"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Build a classification neural network model with hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    - hp (HyperParameters): Keras Tuner's hyperparameter object.\n",
        "\n",
        "    Returns:\n",
        "    - model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    input_dim = X_train.shape[1]  # Number of features\n",
        "    output_dim = y_train.shape[1]  # Number of output classes\n",
        "\n",
        "    # Add input layer\n",
        "    model.add(Dense(\n",
        "        units=hp.Int(\"units_input\", min_value=32, max_value=256, step=32),\n",
        "        activation=hp.Choice(\"activation_input\", [\"relu\", \"tanh\", \"sigmoid\"]),\n",
        "        input_shape=(input_dim,)\n",
        "    ))\n",
        "\n",
        "    # Add 1-3 hidden layers\n",
        "    for i in range(hp.Int(\"num_hidden_layers\", 1, 3)):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=256, step=32),\n",
        "            activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
        "        ))\n",
        "\n",
        "    # Add output layer\n",
        "    model.add(Dense(output_dim, activation=\"softmax\"))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=hp.Choice(\"optimizer\", [\"adam\", \"sgd\"]),\n",
        "        loss=CategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def tune_train_classification_net(X_train, y_train, n_iter, project_name):\n",
        "    \"\"\"\n",
        "    Tunes and trains a classification neural network using Random Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values (one-hot encoded).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try.\n",
        "    - project_name (str): Name for organizing logs and results.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Keras model with the best hyperparameters.\n",
        "    \"\"\"\n",
        "    # Create a RandomSearch tuner\n",
        "    tuner = kt.RandomSearch(\n",
        "        build_model,\n",
        "        objective=\"val_accuracy\",\n",
        "        max_trials=n_iter,\n",
        "        executions_per_trial=3,\n",
        "        directory=\"tuner_logs\",\n",
        "        project_name=project_name\n",
        "    )\n",
        "\n",
        "    # Search for the best hyperparameters\n",
        "    tuner.search(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X-vPiHRMoHH"
      },
      "source": [
        "#### Important notice about the runtime\n",
        "\n",
        "Hyperparameter tuning can take a lot of time, as most Machine Learning algorithms have many hyperparameters, and testing all possible combinations can lead to a combinatorial explosion.\n",
        "\n",
        "To make comparisons fairer, we will limit the hyperparameter search to using no more than **approximately** 30 minutes of computing time.\n",
        "\n",
        "The table above tells us the training time for a single model. For instance, if a random forest takes 4s for the adult dataset, then in 1,800 seconds (30 minutes), we can train 1,800 / 4 = 450 models. Each hyperparameter combination performance will be an average of three repetitions. Thus, we can assess 450 / 3 = 150 hyperparameter combinations.\n",
        "\n",
        "We will control the time using the ``n_iter`` parameter. This parameter defines the maximum number of parameter combinations sampled and tested during the search. Given their smaller number of hyperparameters, some inducers, particularly the trees, may run much faster than 30 minutes.\n",
        "\n",
        "This is a rough approximation based on a single run of the default models. Thus, some models may run faster or slower than 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIIfWK8dMoHI"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your optimised neural net model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    print(\"\\tTuning and training neural net model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_net(X_train, y_train, int(timeout_in_seconds / 3 / results[\"Neural Net\"][dataset][\"Training time\"]), dataset)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwMlBcbDMoHI"
      },
      "source": [
        "### Task 3.2 [8 Marks] - Hyperparameter optimisation for regression Neural Nets\n",
        "\n",
        "Create a function ``tune_train_regression_net(X_train, y_train, n_iter, project_name)`` that uses Keras tuner's ``RandomSearch`` to optimise the hyperparameters of a regression neural net model. ``X_train`` and ``y_train`` are pandas dataframes with the training data and target values. ``n_iter`` is the maximum number of iterations in the random search. ``project_name`` is an identifier used by Keras tuner to save the results on disk.\n",
        "\n",
        "You have the freedom to choose your hyperparameter search space. You can use the same hyperparameter recommendations given for classification.\n",
        "\n",
        "Your function should return the Keras model that achieved the best performance in a validation set of 20% of the training data. Average performance over three runs (``executions_per_trial=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8EA23FsMoHI"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "def build_regression_model(hp):\n",
        "    \"\"\"\n",
        "    Build a regression neural network model with hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    - hp (HyperParameters): Keras Tuner's hyperparameter object.\n",
        "\n",
        "    Returns:\n",
        "    - model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    input_dim = X_train.shape[1]  # Number of features\n",
        "\n",
        "    # Add input layer\n",
        "    model.add(Dense(\n",
        "        units=hp.Int(\"units_input\", min_value=32, max_value=256, step=32),\n",
        "        activation=hp.Choice(\"activation_input\", [\"relu\", \"tanh\", \"sigmoid\"]),\n",
        "        input_shape=(input_dim,)\n",
        "    ))\n",
        "\n",
        "    # Add 1-3 hidden layers\n",
        "    for i in range(hp.Int(\"num_hidden_layers\", 1, 3)):\n",
        "        model.add(Dense(\n",
        "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=256, step=32),\n",
        "            activation=hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
        "        ))\n",
        "\n",
        "    # Add output layer\n",
        "    model.add(Dense(1, activation=\"linear\"))  # Output layer for regression\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=hp.Choice(\"optimizer\", [\"adam\", \"sgd\"]),\n",
        "        loss=MeanSquaredError(),\n",
        "        metrics=[\"mse\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def tune_train_regression_net(X_train, y_train, n_iter, project_name):\n",
        "    \"\"\"\n",
        "    Tunes and trains a regression neural network using Random Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values.\n",
        "    - n_iter (int): Number of hyperparameter configurations to try.\n",
        "    - project_name (str): Name for organising logs and results.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Keras model with the best hyperparameters.\n",
        "    \"\"\"\n",
        "    # Create a RandomSearch tuner\n",
        "    tuner = kt.RandomSearch(\n",
        "        build_regression_model,\n",
        "        objective=\"val_mse\",\n",
        "        max_trials=n_iter,\n",
        "        executions_per_trial=3,\n",
        "        directory=\"tuner_logs\",\n",
        "        project_name=project_name\n",
        "    )\n",
        "\n",
        "    # Search for the best hyperparameters\n",
        "    tuner.search(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS8VRZPMMoHI"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your neural net model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    print(\"Tuning and training neural net model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_net(X_train, y_train, int(timeout_in_seconds / 3 / results[\"Neural Net\"][dataset][\"Training time\"]), dataset)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_mse = model.evaluate(X_test, y_test)\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Prediction quality\"] = 1 - test_mse\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmVYlZTXMoHI"
      },
      "source": [
        "### Task 3.3 [4 Marks] - Hyperparameter optimisation for decision trees\n",
        "\n",
        "We will train the decision trees with hyperparameter optimisation. Our code will implement the search using the ``RandomizedSearchCV`` class.\n",
        "\n",
        "We will create the function ``tune_train_classification_tree(X_train, y_train, n_iter)``, which optimises hyperparameters and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Here are some suggestions:\n",
        "- Maximum tree depth from 10 to 40 with increments of 10. Include None, too.\n",
        "- Minimum samples in a split: 2, 5, 10, 20.\n",
        "- Minimum samples in a leaf node: 1, 2, 5, and 10.\n",
        "- Splitting criteria: gine and entropy.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such a combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhuadBsVMoHJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def tune_train_classification_tree(X_train, y_train, n_iter):\n",
        "    \"\"\"\n",
        "    Tunes and trains a Decision Tree for classification using Randomized Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame or ndarray): Training features.\n",
        "    - y_train (DataFrame or ndarray): Training target values (class labels).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try during the search.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Decision Tree classifier with the best-found hyperparameters.\n",
        "    \"\"\"\n",
        "    # Define the hyperparameter search space\n",
        "    param_distributions = {\n",
        "        \"max_depth\": [10, 20, 30, 40, None],\n",
        "        \"min_samples_split\": [2, 5, 10, 20],\n",
        "        \"min_samples_leaf\": [1, 2, 5, 10],\n",
        "        \"criterion\": [\"gini\", \"entropy\"]\n",
        "    }\n",
        "\n",
        "    # Initialize the Decision Tree Classifier\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "    # Create the RandomizedSearchCV object\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=dt,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=n_iter,\n",
        "        scoring=\"accuracy\",\n",
        "        cv=3,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Perform the search\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Train the model on the entire training set with the best hyperparameters\n",
        "    best_params = random_search.best_params_\n",
        "    print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSPDwAWuMoHJ"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your decision tree model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"\\tTuning and training decision tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_tree(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Decision Tree\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjUbeIpgMoHJ"
      },
      "source": [
        "### Task 3.4 [4 Marks] - Hyperparameter optimisation for regression trees\n",
        "\n",
        "We will train the regression trees with hyperparameter optimisation through the function ``tune_train_regression_tree(X_train, y_train, n_iter)``, which optimises hyperparameters and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You can use the same suggestion for the hyperparameter space provided in the previous task. However, the splitting criteria suitable for regression trees are different. We suggest ``squared_error``, ``friedman_mse``, and ``absolute_error``.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Depls-FMoHJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def tune_train_regression_tree(X_train, y_train, n_iter):\n",
        "    \"\"\"\n",
        "    Tunes and trains a Regression Tree using Randomized Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame or ndarray): Training features.\n",
        "    - y_train (DataFrame or ndarray): Training target values (continuous values).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try during the search.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Regression Tree with the best-found hyperparameters.\n",
        "    \"\"\"\n",
        "    # Define the hyperparameter search space\n",
        "    param_distributions = {\n",
        "        \"max_depth\": [10, 20, 30, 40, None],\n",
        "        \"min_samples_split\": [2, 5, 10, 20],\n",
        "        \"min_samples_leaf\": [1, 2, 5, 10],\n",
        "        \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\"]\n",
        "    }\n",
        "\n",
        "    # Initialize the Decision Tree Regressor\n",
        "    regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "    # Create the RandomizedSearchCV object\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=regressor,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=n_iter,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        cv=3,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Perform the search\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Train the model on the entire training set with the best hyperparameters\n",
        "    best_params = random_search.best_params_\n",
        "    print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW7RZFN-MoHK"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your regression model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_tree(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Decision Tree\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hooGKHJQMoHK"
      },
      "source": [
        "### Task 3.5 [4 Marks] - Hyperparameter optimisation for decision forest\n",
        "\n",
        "We will create the function ``tune_train_classification_forest(X_train, y_train, n_iter)``, which optimises hyperparameters for a classification random forest and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Here are some suggestions:\n",
        "- Number of estimators (trees): 50, 100, 200.\n",
        "- Maximum tree depth from 10 to 40 with increments of 10. Include None, too.\n",
        "- Minimum samples in a split: 2, 5, 10, 20.\n",
        "- Minimum samples in a leaf node: 1, 2, 5, and 10.\n",
        "- Splitting criteria: gine and entropy.\n",
        "- Bootstrap sampling: yes and no.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV_oEVFdMoHK"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def tune_train_classification_forest(X_train, y_train, n_iter):\n",
        "    \"\"\"\n",
        "    Tunes and trains a Random Forest classifier using Randomized Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame or ndarray): Training features.\n",
        "    - y_train (DataFrame or ndarray): Training target values (class labels).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try during the search.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Random Forest classifier with the best-found hyperparameters.\n",
        "    \"\"\"\n",
        "    # Define the hyperparameter search space\n",
        "    param_distributions = {\n",
        "        \"n_estimators\": [50, 100, 200],\n",
        "        \"max_depth\": [10, 20, 30, 40, None],\n",
        "        \"min_samples_split\": [2, 5, 10, 20],\n",
        "        \"min_samples_leaf\": [1, 2, 5, 10],\n",
        "        \"criterion\": [\"gini\", \"entropy\"],\n",
        "        \"bootstrap\": [True, False]\n",
        "    }\n",
        "\n",
        "    # Initialize the Random Forest Classifier\n",
        "    forest = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Create the RandomizedSearchCV object\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=forest,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=n_iter,\n",
        "        scoring=\"accuracy\",\n",
        "        cv=3,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Perform the search\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Train the model on the entire training set with the best hyperparameters\n",
        "    best_params = random_search.best_params_\n",
        "    print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34bU11tHMoHK"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"\\tTuning and training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_forest(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Random Forest\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po7V4DGxMoHK"
      },
      "source": [
        "### Task 3.6 [4 Marks] - Hyperparameter optimisation for regression forest\n",
        "\n",
        "We will create the function ``tune_train_regression_forest(X_train, y_train, n_iter)``, which optimises hyperparameters for a regression random forest and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Our recommendations are similar for the classification forest. However, the splitting criteria suitable for regression problems are ``squared_error``, ``friedman_mse``, and ``absolute_error``.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-4kZS49MoHL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def tune_train_regression_forest(X_train, y_train, n_iter):\n",
        "    \"\"\"\n",
        "    Tunes and trains a Random Forest regressor using Randomized Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame or ndarray): Training features.\n",
        "    - y_train (DataFrame or ndarray): Training target values (continuous values).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try during the search.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Random Forest regressor with the best-found hyperparameters.\n",
        "    \"\"\"\n",
        "    # Define the hyperparameter search space\n",
        "    param_distributions = {\n",
        "        \"n_estimators\": [50, 100, 200],\n",
        "        \"max_depth\": [10, 20, 30, 40, None],\n",
        "        \"min_samples_split\": [2, 5, 10, 20],\n",
        "        \"min_samples_leaf\": [1, 2, 5, 10],\n",
        "        \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\"],\n",
        "        \"bootstrap\": [True, False]\n",
        "    }\n",
        "\n",
        "    # Initialize the Random Forest Regressor\n",
        "    forest = RandomForestRegressor(random_state=42)\n",
        "\n",
        "    # Create the RandomizedSearchCV object\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=forest,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=n_iter,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        cv=3,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Perform the search\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Train the model on the entire training set with the best hyperparameters\n",
        "    best_params = random_search.best_params_\n",
        "    print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLwEaFI_MoHL"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_forest(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Random Forest\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mgOjl7PMoHL"
      },
      "source": [
        "The next cell tabulates all the results. HO stands for Hyperparameter Optimisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB2jBhJOMoHL"
      },
      "outputs": [],
      "source": [
        "print_results_table(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRBcXHb3MoHL"
      },
      "source": [
        "Congratulations! You have reached the end of the assignment. In the remaining of this document, you will analyse the results in a report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tioDTpL4MoHL"
      },
      "source": [
        "## Task 4 [13 Marks] - Report\n",
        "\n",
        "Write a report with less than 1,000 words (around two pages) in the following cells using markdown. You can include graphs and tables in your report. Answer the following questions in your report.\n",
        "\n",
        "- [3 Marks] Discuss the performance of the algorithms in terms of prediction quality and training time. Use plots to compare these methods. Is there a method that stands out?\n",
        "- [3 Marks] Do you think any of the seven hypotheses (machine learning wisdom and misconceptions) presented at the beginning of this assignment are correct? Have you observed any evidence that supports them?\n",
        "- [3 Marks] Is the hyperparameter optimisation worth the time spent? Did you observe significant improvements in prediction quality?\n",
        "- [2 Marks] We have measured the training time of these models, but another important aspect is the inference time. Would you expect some models to be more efficient than others for inference? What is the importance of having efficient models for inference? What is the importance of having efficient models for training?\n",
        "- [2 Marks] The credit card dataset is imbalanced; in this situation, the error rate tends to be very small and difficult to interpret. Extend the performance analysis in this dataset to include the confusion matrix and F1 score. Analyse the performance of the classifiers under these performance measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1nm2TN0MoHM"
      },
      "source": [
        "Use one or more cell here to write your report."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}